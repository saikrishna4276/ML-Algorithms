# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSwJGtzoxAFZlW8itoyHW0NtJSze1Pk7
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import jax
import jax.numpy as jnp
from matplotlib import cm
from sklearn.linear_model import LinearRegression as LR
np.random.seed(45)

class LinearRegression():
  def __init__(self, fit_intercept):
    # Initialize relevant variables
    '''
        :param fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).
    '''
    self.fit_intercept = fit_intercept 
    self.coef_ = None #Replace with numpy array or pandas series of coefficients learned using using the fit methods
    self.all_coef=[] # Stores the thetas for every iteration (theta vectors appended) (for the iterative methods)
    pass
  

  def fit_sklearn_LR(self, X, y):
    # Solve the linear regression problem by calling Linear Regression
    # from sklearn, with the relevant parameters
    model = LR(fit_intercept=self.fit_intercept)
    model.fit(X, y)
    self.coef_ = model.coef_
    if self.fit_intercept:
      self.coef_=np.insert(self.coef_,0,model.intercept_,axis=0)
    self.coef_ = self.coef_.reshape(-1,1)
    return self.coef_
    #print(self.coef_)
    pass
  
  def fit_normal_equations(self, X:pd.DataFrame, y:pd.Series):
    # Solve the linear regression problem using the closed form solution
    # to the normal equation for minimizing ||Wx - y||_2^2
    X_fit = X.copy(deep=True)
    if self.fit_intercept:
      X_fit.insert(loc = 0, column = -1 , value = np.ones(X.shape[0]))
    X_fit = X_fit.to_numpy()
    y = y.to_numpy()
    X_T = X_fit.transpose()
    self.coef_ = np.linalg.inv(X_T @ X_fit) @ (X_T @ y)
    self.coef_ = self.coef_.reshape(-1,1)
    #print(self.coef_)
    pass

  def fit_SVD(self, X, y):
    # Solve the linear regression problem using the SVD of the 
    # coefficient matrix
    X_fit = X.copy(deep=True)
    if self.fit_intercept:
      X_fit.insert(loc = 0, column = -1 , value = np.ones(X.shape[0]))
    U,S,Vt = np.linalg.svd(X_fit, full_matrices=False)
    self.coef_ =  Vt.transpose() @ np.linalg.inv(np.diag(S)) @ U.transpose() @ y
    self.coef_ = self.coef_.reshape(-1,1)
    pass

  def mse_loss(self, theta, X, y):                
    # Compute the MSE loss with the learned model
    #mse = jnp.mean((y-(X @ theta)).T @ (y-(X @ theta)))
    mse = jnp.mean(((X @ theta) - y)**2)
    return mse
    pass

  def compute_gradient(self, X, y, u, penalty='unregularized'):
    # Compute the analytical gradient (in vectorized form) of the 
    # 1. unregularized mse_loss,  and 
    # 2. mse_loss with ridge regularization
    # penalty :  specifies the regularization used  , 'l2' or unregularized
    # X_fit = X.copy(deep=True)
    # if self.fit_intercept:
    #   X_fit.insert(loc = 0, column = -1 , value = np.ones(X.shape[0]))
    # X_fit = X_fit.to_numpy()
    # y = y.to_numpy()
    theta = self.coef_
    if penalty == 'unregularized':
      mse_grad = (2/X.shape[0]) * (X.T @ ((X @ theta) - y))
      #print(mse_grad)
    else:
      mse_grad = (2/X.shape[0]) * (X.T @ ((X @ theta) - y))  + 2*u*theta
    return mse_grad

  def compute_jax_gradient(self, X, y, u, penalty = 'unregularized'):
    # Compute the gradient of the 
    # 1. unregularized mse_loss, 
    # 2. mse_loss with LASSO regularization and 
    # 3. mse_loss with ridge regularization, using JAX 
    # penalty :  specifies the regularization used , 'l1' , 'l2' or unregularized
    # X_fit = X.copy(deep=True)
    # if self.fit_intercept:
    #   X_fit.insert(loc = 0, column = -1 , value = np.ones(X.shape[0]))
    # theta = self.coef_
    # X_fit = X_fit.to_numpy()
    # y = y.to_numpy()
    if penalty == 'unregularized':
      def p(theta):
        return 0.0
    elif penalty == 'l2':
      def p(theta):
        return jnp.trace(jnp.dot(theta.T,theta))
    elif penalty == 'l1':
      def p(theta):
        return jnp.sum(jnp.abs(theta))
    mse_grad1 = jax.grad(self.mse_loss,argnums=0)
    mse_grad2 = jax.grad(p)
    theta = self.coef_
    mse_grad = mse_grad1(theta,X,y) + u*mse_grad2(theta)
    return mse_grad
    
    pass

  def mini_batch(self, data: np.ndarray, batch_size=0):
    mini_batch = []

    for i in range(0, data.shape[0], batch_size):
      mini_batch.append(data[i:i+batch_size])
    # print(mini_batch)
    # exit()
    return mini_batch


  def fit_gradient_descent(self, X, y, batch_size, gradient_type, penalty_type, penalty_value = 0.05, num_iters=20, lr=0.05):
    # Implement batch gradient descent for linear regression (should unregularized as well as 'l1' and 'l2' regularized objective)
    # batch_size : Number of training points in each batch
    # num_iters : Number of iterations of gradient descent
    # lr : Default learning rate
    # gradient_type : manual or JAX gradients
    # penalty_type : 'l1', 'l2' or unregularized
    X_train = X.copy(deep=True)
    if self.fit_intercept:
      X_train.insert(loc = 0, column = -1 , value = np.ones(X.shape[0]))
    if isinstance(X_train,pd.DataFrame):
      X_train = X_train.to_numpy()
    if isinstance(y,pd.Series):
      y = y.to_numpy()
    if y.shape == (X_train.shape[0],):
      y = y.reshape(-1,1)
    #print(y.shape)
    data = np.hstack((X_train,y))
    self.coef_ = np.zeros((X_train.shape[1],1),dtype = float)
    self.all_coef.append(self.coef_)
    for i in range(num_iters):
      #np.random.seed(42)
      #np.random.shuffle(data)
      # print(data.shape)
      # print(type(data))
      mini_batch = self.mini_batch(data = data, batch_size = batch_size)
      for mini in mini_batch:
        x_train = mini[:,:-1]
        y_train = mini[:,-1]
        y_train = y_train.reshape(-1,1)
        if gradient_type == 'manual':
          self.coef_ = self.coef_ - lr * self.compute_gradient(x_train,y_train,penalty_value,penalty_type)
        else:
          #print(i)
          self.coef_ = self.coef_ - lr * self.compute_jax_gradient(x_train,y_train,penalty_value,penalty_type)
      
      self.all_coef.append(self.coef_)
    #print('final coef' , self.coef_)



  def fit_SGD_with_momentum(self, X, y, batch_size = 1, penalty_type='l2', penalty_value = 0.05,beta=0.9, lr = 0.01, num_iters=20, gradient_type='manual'):
    # Solve the linear regression problem using sklearn's implementation of SGD
    # penalty: refers to the type of regularization used (ridge)
    X_train = X.copy(deep=True)
    if self.fit_intercept:
      X_train.insert(loc = 0, column = -1 , value = np.ones(X.shape[0]))
    if isinstance(X_train,pd.DataFrame):
      X_train = X_train.to_numpy()
    if isinstance(y,pd.Series):
      y = y.to_numpy()
    if y.shape == (X_train.shape[0],):
      y = y.reshape(-1,1)
    data = np.hstack((X_train,y))
    self.coef_ = np.ones((X_train.shape[1],1),dtype = float)
    change = 0.0
    for i in range(num_iters):
      #print(i)
      np.random.shuffle(data)
      # print(data.shape)
      # print(type(data))
      mini_batch = self.mini_batch(data = data, batch_size = batch_size)
      for mini in mini_batch:
        x_train = mini[:,:-1]
        y_train = mini[:,-1]
        if gradient_type == 'manual':
          new_change = lr * self.compute_gradient(x_train,y_train,penalty_value,penalty_type) + beta * change
          self.coef_ = self.coef_ - new_change
          change = new_change
        else:
          #print(i)
          new_change = self.compute_jax_gradient(x_train,y_train,penalty_value,penalty_type) + beta * change
          self.coef_ = self.coef_ - lr  * new_change
          change = new_change
    pass

  def predict(self, X:pd.DataFrame):
    # Funtion to run the LinearRegression on a test data point
    if isinstance(X, np.ndarray):
      X = pd.DataFrame(X)
    X_test  = X.copy(deep=True)
    if self.fit_intercept:
      X_test.insert(loc = 0, column = -1 , value = np.ones(X.shape[0]))
    #print(X_test.shape, self.coef_.shape)
    y =  X_test @ self.coef_
   # print(y.shape)
    return y.sum(axis=1)
    pass


  def plot_surface(self, X, y, theta_0, theta_1):
    '''
    Function to plot RSS (residual sum of squares) in 3D. A surface plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1 by a
    red dot. Uses self.coef_ to calculate RSS. Plot must indicate error as the title.
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to indicate RSS #pd Series of all theta_0
      :param theta_1: Value of theta_1 for which to indicate RSS #pd Series of all theta_1
      :return matplotlib figure plotting RSS
    '''
    coef0 = theta_0
    coef1 = theta_1
    theta_0 = 8
    theta_1 = 3 
    x = np.linspace(-1,1,50)
    y = theta_0 + theta_1*x
    #print(theta_0, theta_1)
    def cost_func(theta_0, theta_1):
      theta_0 = np.atleast_3d(np.asarray(theta_0))
      theta_1 = np.atleast_3d(np.asarray(theta_1))
      return np.average((y -  (theta_0 + theta_1*x))**2)
    
    theta0_vals = np.linspace(self.coef_[0]-10, self.coef_[0]+10, 100)
    theta1_vals = np.linspace(self.coef_[1]-4, self.coef_[1]+4, 100)
    b,m = np.meshgrid(theta0_vals, theta1_vals)
    zs = np.array([cost_func(bp,mp) for bp,mp in zip(np.ravel(b), np.ravel(m))])
    Z = zs.reshape(m.shape)
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(b, m, Z, rstride=1, cstride=1, cmap=cm.coolwarm, alpha=0.7)
    k = cost_func(coef0, coef1)
    ax.scatter([coef0],[coef1],[k], c='r', s=25, marker='.')
    ax.set_xlabel('theta_0')
    ax.set_ylabel('theta_1')
    ax.set_zlabel('Error')
    plt.title("Error:"+str(k))
    return fig
    
    pass

  def plot_line_fit(self, X, y, theta_0, theta_1):
    """
    Function to plot fit of the line (y vs. X plot) based on chosen value of theta_0, theta_1. Plot must
    indicate theta_0 and theta_1 as the title.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting line fit
    """
    plt.clf()
    X_row=np.array(X)
    y_row=np.array(y)
    plt.scatter(X,y)
    X_line=np.linspace(np.min(X_row),np.max(X_row),5)
    y_line=X*theta_1+theta_0
    plt.plot(X,y_line, color='red')
    plt.xlabel("Features")
    plt.ylabel("Output")
    plt.title(f"Line fit plot for theta_0:{theta_0} theta_1:{theta_1}")
    return plt


  def plot_contour(self, X, y, theta_0, theta_1):
    """
    Plots the RSS as a contour plot. A contour plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1, and the
    direction of gradient steps. Uses self.coef_ to calculate RSS.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting the contour
    """
    # theta_0_grid, theta_1_grid = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))
    # RSS_grid = np.zeros(theta_0_grid.shape)
    # for i in range(theta_0_grid.shape[0]):
    #     for j in range(theta_0_grid.shape[1]):
    #         RSS_grid[i, j] = np.mean((y - theta_0_grid[i, j] - theta_1_grid[i, j]*X)**2)
    
    # # Create a figure and axis object
    # fig, ax = plt.subplots()
    
    # # Plot the contour lines for the RSS
    # ax.contour(theta_0_grid, theta_1_grid, RSS_grid, levels=np.logspace(-1, 3, 10))
    
    # # Plot the starting point as a red dot
    # ax.plot(theta_0, theta_1, 'ro', markersize=10)
    
    # # Set the title and axis labels
    # ax.set_title('RSS Contour Plot')
    # ax.set_xlabel('theta_0')
    # ax.set_ylabel('theta_1')
    # return fig
    coef0 = theta_0
    coef1 = theta_1
    theta_0 = 8
    theta_1 = 3 
    x = np.linspace(-1,1,50)
    y = theta_0 + theta_1*x
    #print(theta_0, theta_1)
    def cost_func(theta_0, theta_1):
      theta_0 = np.atleast_3d(np.asarray(theta_0))
      theta_1 = np.atleast_3d(np.asarray(theta_1))
      return np.average((y -  (theta_0 + theta_1*x))**2)
    
    theta0_vals = np.linspace(self.coef_[0]-10, self.coef_[0]+10, 100)
    theta1_vals = np.linspace(self.coef_[1]-4, self.coef_[1]+4, 100)
    b,m = np.meshgrid(theta0_vals, theta1_vals)
    zs = np.array([cost_func(bp,mp) for bp,mp in zip(np.ravel(b), np.ravel(m))])
    Z = zs.reshape(m.shape)
    fig, ax = plt.subplots()
    ax.contour(b,m,Z, cmap='viridis')
    
    # Plot the starting point as a red dot
    ax.plot(coef0,coef1, 'ro', markersize=5)
    
    # Set the title and axis labels
    ax.set_title('RSS Contour Plot')
    ax.set_xlabel('theta_0')
    ax.set_ylabel('theta_1')
    
    # Return the figure object
    return fig